To measure defect prediction results, we employ three different metrics: \textit{Precision}, \textit{Recall} and \textit{F1}. These evaluation metrics are widely used to evaluate the performance of defect prediction~\cite{menzies2007data, menzies2010defect, nam2013transfer} as well as information retrieval binary classification~\cite{manning2008introduction}. Typically, precision is the fraction of retrieved instances that are relevant, recall is the fraction of relevant instances that are retrieved, whereas F1 combines both precision and recall to measure the performance of our model. Below is the equation of these metrics:
\begin{equation}
\label{eq:precision}
Precision = \frac{TP}{TP+FP}
\end{equation}

\begin{equation}
\label{eq:recall}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
\label{eq:f1}
F1 = \frac{2 * Precision * Recall}{Precision + Recall}
\end{equation}
where \textit{TP}, \textit{FP}, and \textit{FN} are considered as true positive, false positive, and false negative respectively. True positive is the number of predicted defective files that are truly defective, while false positive is the number of predicted defective files that are actually not defective. False negative records the number of predicted non-defective files that are actually defective. A higher precision makes the manual inspection on a certain amount of predicted defective files find more defects, while an increase in recall can reveal more defects given a project. F1 takes consideration of both precision and recall.
