To measure defect prediction performance, we employ three different evaluation metrics: \textit{Precision}, \textit{Recall} and \textit{F1}. These metrics are widely used to evaluate the performance of defect prediction~\cite{menzies2007data, menzies2010defect, nam2013transfer}. % as well as information retrieval binary classification~\cite{manning2008introduction}. 
%Precision is the fraction of predicted defective files that are actually defective, recall is the fraction of actual defective files that are predicted as defective, whereas F1 combines both precision and recall to measure the performance of our model. 
Below is the equation for each of these metrics:
\begin{equation}
\label{eq:precision}
Precision = \frac{TP}{TP+FP}
\end{equation}

\begin{equation}
\label{eq:recall}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
\label{eq:f1}
F1 = \frac{2 * Precision * Recall}{Precision + Recall}
\end{equation}
where \textit{TP}, \textit{FP}, and \textit{FN} are considered as true positive, false positive, and false negative, respectively. True positive is the number of predicted defective files that are truly defective, while false positive is the number of predicted defective files that are actually not defective. False negative records the number of predicted non-defective files that are actually defective. A higher precision makes the manual inspection on a certain amount of predicted defective files find more defects, while an increase in recall can reveal more defects given a project. F1 takes a consideration of both precision and recall.
