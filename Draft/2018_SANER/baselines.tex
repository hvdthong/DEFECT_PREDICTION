%To evaluate the performance of our approach in defect prediction, we compare with traditional features (see Section~\ref{sec:relatedwork}). The first baseline consists some traditional features, including lines of code, cyclomatic complexity~\cite{mccabe1976complexity}, total number of methods, total number of public methods, total number of local methods, the depth of inheritance tree, comment density, etc. These features are well described in~\cite{hassan2009predicting} and have been widely used in previous work~\cite{jing2014dictionary, menzies2007data, menzies2010defect, nam2013transfer, zimmermann2007predicting, yang2015deep}. Since they are popular features so we can directly compare our work with previous studies. We note that the traditional features do not contain Abstract Syntax Tree (AST) nodes, which are described in this paper. 

We compare our approach with the defect prediction models constructed based on two traditional features. The first traditional features are embedding features generated following Wang et al.~\cite{wang2016automatically}. The second traditional features are AST features extracted from source code's AST. Specifically, we collect AST nodes from source code and represent the source code as a vector of term frequencies of the AST nodes. These two baselines were shown their effectiveness in solving defect prediction problem~\cite{wang2016automatically}.

%To evaluate the performance of our approach in defect prediction, we compare it with traditional defect prediction models that are constructed using the state-of-the-art semantic features learning approach by Wang et al.~\cite{wang2016automatically}. They employ Deep Belief Network~\cite{hinton2009deep} to automatically learn semantic features from token vectors extracted from source code's AST. They have shown that their semantic features significantly improve the performance of defect prediction. 

%The second baseline includes semantic features constructed following Wang et al. approaches~\cite{wang2016automatically}. Typically, they tried to employ deep belief network~\cite{hinton2009deep} to automatically learn semantic features from token vectors extracted from programs' AST. They also proved that their semantic features significantly improve the performance of defect prediction in software engineering domain. 

We employ three popular machine learning algorithms to build defect prediction models for each traditional features. These algorithms are widely used in software engineering~\cite{wang2016automatically, jing2014dictionary} described as follows: 
\begin{itemize}
%	\item Decision tree is used to build a tree-based predictive model where branch nodes represent an option on feature values while leaf nodes represent predicted values. Tree models in which the predicted values are taken from a finite set are called classification trees. In these tree structures, leaf nodes represent class labels and branch node sequences represent conjunctions of features that lead to those class labels. This algorithm is very popular in statistics, data mining and machine learning~\cite{safavian1991survey}.
	\item Decision tree is used to build a tree-based classification model where branch nodes represent an option on feature values while leaf nodes represent predicted values~\cite{safavian1991survey}.
%	\item Logistic regression is a predictive model that explains the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Logistic regression is used in various applications like: health, statistics, data analysis, etc.~\cite{hosmer2013applied}.
	\item Logistic regression is a well-known classification model is employed in various application such as: health, statistics, data analysis, etc.~\cite{hosmer2013applied}. 
%	explains the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Logistic regression is used in various applications like: health, statistics, data analysis, etc.~\cite{hosmer2013applied}.
	\item Na\"{i}ve Bayes classifier, which is highly scalable, is a simple probabilistic classifiers based on applying Bayes' theorem~\cite{vapnik1998statistical}. 
%	with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring only a number of parameters that is linear with the number of variables (features/predictors). For some types of probability models, Naive Bayes classifiers can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for Naive Bayes models uses the method of maximum likelihood~\cite{pan2002maximum}.
\end{itemize}

%The 20 traditional features are available for PROMISE data, and the work from He et al. [13] contains the full list of the 20 features, which are well described in their Table II. These features and data have been widely used in previous work [20, 33, 34, 42, 70]. We choose the widely used PROMISE data so that we can directly com- pare our work with previous studies. Note that, for a fair comparison, we also perform the noise removal approach de- scribed in Section 3.2.1 on the PROMISE data.


%The traditional features from PROMISE do not contain AST nodes, which were used by our DBN models. For a fair comparison, our second baseline of traditional features is the AST nodes that were given to our DBN models, i.e., the AST nodes in all files after fixing noise (Section 3.2.1). Each instance is represented as a vector of term frequencies of the AST nodes.

%To bridge the gap between programs' semantics and
%defect prediction features, this paper proposes to leverage a
%powerful representation-learning algorithm, deep learning,
%to learn semantic representation of programs automatically
%from source code. Specically, we leverage Deep Belief
%Network (DBN) to automatically learn semantic features
%from token vectors extracted from programs' Abstract
%Syntax Trees (ASTs).
%Our evaluation on ten open source projects shows that
%our automatically learned semantic features signicantly improve
%both within-project defect prediction (WPDP) and
%cross-project defect prediction (CPDP) compared to traditional
%features. Our semantic features improve WPDP on
%average by 14.7% in precision, 11.5% in recall, and 14.2%
%in F1. For CPDP, our semantic features based approach
%outperforms the state-of-the-art technique TCA+ with traditional
%features by 8.9% in F1.