This section presents our experimental results. We examine the performance of our proposed DDA approach in both within-project and cross-project defect prediction settings. In within-project setting, we use the source code of an older version of a project to construct the DDA model and evaluate the model based on the source code of the newer version of the project. In the cross-project setting, we randomly pick one project as a source project to build the DDA model and use the model to predict defects for a target project that is randomly picked from a set of projects that excludes the source project.

We answer the following research questions: 

\textbf{RQ1: In within-project defect prediction, does our proposed approach outperform the baselines?}

%We run experiments on four popular software projects. For each project, we construct defect prediction models using the training version and evaluate them in the testing version. Note that the training and testing are collected in two different timelines (i.e., version as of January $1^{st}$, 2015 and version as of July $1^{st}$, 2015).
%, each of which uses two versions of the same project collected in two different time-lines (see Table~\ref{tab:data}). The training data, which is the older version of these projects, are used to construct defect prediction models, and the testing data is used to evaluate the performance of our prediction models. 
Table~\ref{tab:within} shows the precision, recall and F1 score of different defect prediction models. The highest F1 scores are highlighted in bold. For example, the F1 score of our approach is 46.7\% for Traccar project, while the best F1 score is only 23.9\% for approaches that use embedding features (using decision tree), and the best F1 score is 20.7\% for approaches that use AST features (using logistic regression). On average, the best baseline that uses AST features achieves an F1 score of 34.9\%, while the best baseline that uses embedding features constructed following Wang et al.~\cite{wang2016automatically} approach achieves an F1 score of 43.8\%. Our DDA approach beats these two baselines by achieving an F1 score of 52.4\%. The result demonstrates that we can improve the F1 score by 19.63\% when compared with the best baseline. 
%to be 8.6\% higher when compared with the best baseline. 


\textbf{RQ2: In cross-project defect prediction, does our proposed approach outperform the baselines?}

%We also compare our DSSL model against Wang et al.~\cite{wang2016automatically} approach. Note that we also provide a bench-mark of within-project for a fair comparison. 
We evaluate eight pairs of projects. For each pair, we take two different projects for training and testing. Table~\ref{tab:cross} presents the precision, recall and F1 scores of our proposed method (DDA) vs. best defect prediction model constructed using embedding features. Note that we employ different machine learning algorithms (i.e., decision tree, logistic regression, na\"{i}ve Bayes) to build defect prediction model from embedding features. The results show that na\"{i}ve Bayes achieves the best results in cross-project setting, which is similar to findings in Wang et al. work~\cite{wang2016automatically}. Due to this reason, we only compare DDA to a defect prediction model that is built using na\"{i}ve Bayes algorithm.
%We employ na\"{i}ve Bayes algorithm to build defect prediction model from embedding features since this algorithm achieves the best F1 score in the within-project setting (see Table~\ref{tab:within}).
In the table, the best F1 scores are highlighted in bold. For example, when the source project is Nuvolabase (training) and the target project is Checkstyle (testing), our DDA achieves an F1 score of 66.7\% whereas the best defect prediction model using embedding features only achieves an F1 score of 45.3\%. In average, DDA achieves an F1 score of 36.4\%, which outperforms the best model that uses embedding features by 18.95\%. 

\textbf{RQ3: What is the training time of proposed approach?}
We run experiments on a NVIDIA DGX-1~\cite{nvidia} machine to construct the DDA model. 
We keep track of the training time that our server needs to build the DDA model on the four software projects in the within-project setting. 
Table~\ref{tab:time} shows the training time to build the DDA model. In average, the training time for our proposed approach varies from 5.67 seconds (Traccar) to 62.5 seconds (Checkstyle). On average, it takes 34.4 seconds to build the DDA model. It shows that our DDA should be applicable in practice.

%To answer this question, we use two different features to build defect prediction models. We run the experiments on 28 sets of software project, each of which uses two versions of the same project (see Table~\ref{tab:data}). The training data, which is the older version of these projects, are used to construct defect prediction models, and the testing data is used to evaluate the performance of our prediction models. Table~\ref{tab:dttree} shows the precision, recall and F1 of the defect prediction experiments. On average, code features achieve a F1 of 0.354, the semantic features constructed following~\cite{wang2016automatically} approaches achieve a F1 of 0.456, and our approach achieves a F1 of 0.510. The results demonstrate that we can improve the defect prediction F1 by 5.4\% on average on 28 software projects. 

%We also use code features and semantic features separately to build defect prediction models by using two alternative classification algorithm, i.e., logistic regression and Naive Bayes. Table~\ref{tab:lr} shows the precision, recall and F1 scores on SSA vs. two different defect prediction models constructed by code features and semantic features using logistic regression. On average, code features and semantic features achieve F1 of 0.421 and 0.432 respectively. It shows that our SSA model improve the performance of F1 by 8.9\% and 7.8\% compared to two defect prediction models built using logistic regression algorithm. Table~\ref{tab:nb} presents the results of defect prediction models code features and semantic features using Naive Bayes algorithm, and SSA approaches. We see that our SSA outperforms these two baseline approaches. Table~\ref{tab:all} shows the F1 results of our approach compared to the two code features and semantic features constructed be three different machine learning algorithms, i.e., decision tree, logistic regression, and Naive Bayes. It shows that SSA outperforms the other approaches on 28 software projects in average. 

%The results demonstrate
%that by using the semantic features automatically learned
%by DBN instead of the PROMISE features, we can improve
%the defect prediction F1 by 14.2% on average on 16 data
%sets. The average improvement in the precision and recall is
%14.7% and 11.5% respectively.
%Since the DBN algorithm has randomness, the generated
%features vary between dierent runs. Therefore, we run our
%DBN-based feature generation approach ve times for each
%experiment. Among the runs, the dierence in the generated
%features is at the level of 1.0e-20, which is too small to
%propagate to precision, recall, and F1. In other words, the
%precision, recall, and F1 of all ve runs are identical.

