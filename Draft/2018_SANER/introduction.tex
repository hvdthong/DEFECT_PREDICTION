%Recently, text retrieval ~\cite{Salton:1988:TAA:54259.54260} approaches have been widely used to support a lof of software engineering task~\cite{Unterkalmsteiner:2016:LIR:3023163.3023241, Haiduc:2013:AQR:2486788.2486898, Arnaoudova:2015:UTR:2819009.2819224}. In these approaches, the lexical gap between user queries and code is usually identified as significant issues~\cite{Poshyvanyk:10.1109/TSE.2011.84}. In bug localization 

Software defect prediction techniques~\cite{hassan2009predicting, jiang2013personalized} have been developed to automatically detect defects in program elements (e.g., files, changes, and methods), which in turn helps developers reduce their testing efforts and minimizes
%to help developers reduce their testing efforts, thus lowering 
software development costs. 
In a defect prediction task, one typically constructs
%Defect prediction tries to construct 
defect prediction models from software history, and uses these models to predict whether new instances of program elements contain defects. 
%Traditional approaches try to construct accurate defect prediction models following two different directions: 
Traditionally, research efforts to construct accurate defect prediction models fall into two directions:
the first direction focuses on manually designing a set of discriminative features that can represent defects more effectively; the second direction aims to apply a new machine learning algorithm~\cite{bishop2006pattern} that improves conventional prediction models. 

In the first direction, most researchers manually construct features to discriminate buggy source files from non-buggy ones. Typically, the constructed features reflect changes in source code (i.e., the number of lines of code added, removed, etc.), complexity of the code, or understandability of the code~\cite{jiang2013personalized, harrison1998evaluation}. 
%Meanwhile, machine learning algorithms have been used to construct a defect prediction model, such as decision tree, logistic regression, and na\"{i}ve Bayes, etc~\cite{jing2014dictionary}. 
%However, traditional approaches often fail to capture the semantic differences of programs since they cannot learn code structure of different semantics. 
A common drawback of approaches in this direction is that the constructed features cannot adequately capture the semantics of different programs~\cite{wang2016automatically}. For example, two Java programs may have identical number of \texttt{if} and \texttt{for} statements, but these statements are used in different contexts. Consider two programs having one if statement and one for statement. In the first program, the if statement is called inside the for statement while in the second program the for statement is called inside the if statement.  In this case, although the two program files have different semantics, the constructed features (i.e., number of if and for statements) are identical  and thus fails to distinguish the difference of the two programs. As such, these kinds of features are suboptimal.

%McCabe et al.~\cite{mccabe1976complexity} features focus on a complexity measure for the program elements, CK features~\cite{chidamber1994metrics} based on function and inheritance counts to understand the development of software projects, whereas MOOD features~\cite{harrison1998evaluation} provide an overall assessment of a software system. The other features are constructed based on source code changes like the number of lines of code added, removed, etc.~\cite{jiang2013personalized, e1994candidate}. On the other hand, many machine learning algorithms have been widely used for software defect prediction, including decision tree, logistic regression, Naive Bayes, etc~\cite{jing2014dictionary}. However, traditional approaches fail to distinguish code regions of different semantics. 

%To bridge the gap between programs' semantic information and features used for defect prediction, 
In view of this limitation, Wang et al.~\cite{wang2016automatically} applied a deep belief network (DBN) model~\cite{hinton2009deep} to automatically learn embedding features (referred to as semantic information) in the form of a compressed representation of token vectors extracted from the program's Abstract Syntax Tree (AST)~\cite{Hindle:2012:NS:2337223.2337322}. The learned features are then utilized as input to train a defect classification model. Still, in this approach, the embedding features and the defect prediction model are built separately.
%builds semantic features and defect prediction model independently. 
That is, the embedding features are learned from source files in an unsupervised manner, without considering the true label. Moreover, tokens are mapped to unique integer identifiers without considering the importance of each token. Hence, the embedding features may be suboptimal. 

To address this shortcoming, we propose a new deep learning approach named \emph{deep discriminative autoencoder} (DDA), which provides an end-to-end learning scheme to construct discriminative embedding features and accurate defect classification model in simultaneously. DDA extends a deep autoencoder model~\cite{Vincent2010}, which is an unsupervised learning model. Our DDA adds a discriminative power to the deep autoencoder model, making it a supervised learning model.
%\textcolor{red}{Unlike a deep autoencoder model~\cite{Vincent2010} that has only encoder and decoder layers to learn embedding features from input data, our proposed DDA approach augments additional hidden layers that map the embedding features to a defect label.} %In this case, the embedding features and defect prediction model are learned from source files and set of labels of these files in one stage, which helps to overcome the drawbacks of embedding features.} 
%Our proposed DDA approach extends the deep autoencoder model~\cite{Vincent2010} by augmenting additional hidden layers that map the embedding features to an output layer where the defect classification is made. 
We summarize the key contributions of this paper below:
\begin{itemize}
	\item We develop a new deep learning approach for defect prediction, which is trained end-to-end using a joint loss function that simultaneously takes into account the defect prediction accuracy and reconstruction quality of the embedding features.
	\item We conduct experiments on four popular Java software projects. The results show that our approach significantly outperforms traditional defect prediction methods by 19.63\% and 18.95\% in terms of F1 score, for within-project and cross project defect prediction tasks, respectively. 
\end{itemize}
% We propose to leverage a powerful representationlearning
%algorithm, namely deep learning, to learn
%semantic features from token vectors extracted from
%programs' ASTs automatically.
% We leverage the semantic features learned automatically
%by DBN to improve both within-project defect
%prediction (WPDP) and cross-project defect prediction
%(CPDP).
% Our evaluation results on ten open source Java projects
%show that the automatically generated semantic features
%improve both WPDP and CPDP. ForWPDP, our
%semantic features achieve an average improvement of
%precision by 14.7%, recall by 11.5%, and F1 by 14.2%
%compared to traditional features. For CPDP, our semantic
%feature based approach outperforms the stateof-
%the-art technique TCA+ [42] built on traditional
%features by 8.9% in F1.

%To bridge the gap between programs' semantic information
%and features used for defect prediction, this paper proposes
%to leverage a powerful representation-learning algorithm,
%namely deep learning [17], to learn semantic represen-
%tation of programs automatically and use the representation
%to improve defect prediction. Specically, we use Deep Belief
%Network (DBN) [16] to automatically learn features from token
%vectors extracted from programs' ASTs, and then utilize
%these features to train a defect prediction model.

%This paper makes the following contributions:
% We propose to leverage a powerful representationlearning
%algorithm, namely deep learning, to learn
%semantic features from token vectors extracted from
%programs' ASTs automatically.
% We leverage the semantic features learned automatically
%by DBN to improve both within-project defect
%prediction (WPDP) and cross-project defect prediction
%(CPDP).
% Our evaluation results on ten open source Java projects
%show that the automatically generated semantic features
%improve both WPDP and CPDP. ForWPDP, our
%semantic features achieve an average improvement of
%precision by 14.7%, recall by 11.5%, and F1 by 14.2%
%compared to traditional features. For CPDP, our semantic
%feature based approach outperforms the stateof-
%the-art technique TCA+ [42] built on traditional
%features by 8.9% in F1.


The remainder of this paper is organized as follows. Section~\ref{sec:framework} elaborates our proposed DDA approach. Section~\ref{sec:exp_results} presents our experimental results, followed by discussion on threats to validity in Section~\ref{sec:threats}. 
%Review of key related works is given in Section~\ref{sec:relatedwork}. 
Finally, Section~\ref{sec:conclusion} concludes this paper.

%Section~\ref{sec:relatedwork} and Section~\ref{sec:conclusion} describe the related work and conclusion of our paper. 
%provides backgrounds on defect prediction and DBN. Section 3 describes our proposed approach to learn semantic features from source code automatically, and leverage these learned features to predict defects. Section 4 shows the experimental setup. Section 5 evaluates the performance of learned semantic features. Section 6 and Section 7 present threats to our work and related work respectively. We conclude this paper in Section 8.

%Programs have well-dened syntax, which can be represented
%by Abstract Syntax Trees (ASTs) [15] and have been
%successfully used to capture programming patterns [44, 46].
%In addition, programs have semantics, which is hidden
%deeply in source code [65]. It has been shown that programs'
%semantic information is useful for tasks such as
%code completion and bug detection [15, 28, 44, 46, 60]. Such
%semantic information should also be useful for characterizing
%defects for improving defect prediction. Specically,
%in order to make accurate predictions, the features need to
%be discriminative: capable of distinguishing one instance of
%code region from another.
%However, existing traditional features cannot distinguish
%code regions of dierent semantics. Program les with
%dierent semantics can have traditional features with the
%same values. For example, Figure 1 shows two Java les,
%File1.java and File2.java, both of which contain an if
%statement, a for statement, and two function calls. Using
%traditional features to represent these two les, their feature
%vectors are identical, because these two les have the same
%source code characteristics in terms of lines of code, function
%calls, raw programming tokens, etc. However, the semantic
%information is dierent. Features that can distinguish such
%semantic dierences should enable the building of more
%accurate prediction models.
%To bridge the gap between programs' semantic information
%and features used for defect prediction, this paper proposes
%to leverage a powerful representation-learning algorithm,
%namely deep learning [17], to learn semantic represen-
%tation of programs automatically and use the representation
%to improve defect prediction. Specically, we use Deep Belief
%Network (DBN) [16] to automatically learn features from token
%vectors extracted from programs' ASTs, and then utilize
%these features to train a defect prediction model.
%
%DBN is a generative graphical model, which learns a representation
%that can reconstruct training data with a high
%probability. It automatically learns high-level representation
%of data by constructing a deep architecture [2]. We have
%seen successful applications of DBN in many elds, including
%speech recognition [37], image classication [6, 25], natural
%language understanding [35, 55], and semantic search [54].
%To use a DBN to learn features from code snippets, we
%convert the code snippets into vectors of tokens with structural
%and contextual information preserved, and use these
%vectors as input to the DBN. For the two code snippets in
%Figure 1, the input vectors will be [..., if, foo, for,
%bar, ...] and [..., foo, for, if, bar, ...] respectively.
%Since the vectors of these two les are dierent,
%DBN will automatically learn features to distinguish these
%two code snippets (details are in Figure 3 and Section 3.3).
%This paper makes the following contributions:
% We propose to leverage a powerful representationlearning
%algorithm, namely deep learning, to learn
%semantic features from token vectors extracted from
%programs' ASTs automatically.
% We leverage the semantic features learned automatically
%by DBN to improve both within-project defect
%prediction (WPDP) and cross-project defect prediction
%(CPDP).
% Our evaluation results on ten open source Java projects
%show that the automatically generated semantic features
%improve both WPDP and CPDP. ForWPDP, our
%semantic features achieve an average improvement of
%precision by 14.7%, recall by 11.5%, and F1 by 14.2%
%compared to traditional features. For CPDP, our semantic
%feature based approach outperforms the stateof-
%the-art technique TCA+ [42] built on traditional
%features by 8.9% in F1.
%The rest of this paper is summarized as follows. Section 2
%provides backgrounds on defect prediction and DBN. Section
%3 describes our proposed approach to learn semantic
%features from source code automatically, and leverage these
%learned features to predict defects. Section 4 shows the experimental
%setup. Section 5 evaluates the performance of
%learned semantic features. Section 6 and Section 7 present
%threats to our work and related work respectively. We conclude
%this paper in Section 8.